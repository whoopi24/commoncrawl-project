{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interdisciplinary Project\n",
    "by Marina Sommer | 11778902"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66af32a14aeb3ff3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Workflow is as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a34fa94bc89a283"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import packages\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import re \n",
    "import random\n",
    "import gzip\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "from warcio import ArchiveIterator\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:42:54.778076600Z",
     "start_time": "2024-02-17T18:42:53.446781200Z"
    }
   },
   "id": "417a92092b5e6dab",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2023-40/indexes/cluster.idx\n"
     ]
    }
   ],
   "source": [
    "# 1) select crawl: CC-MAIN-2023-40 and download cluster.idx\n",
    "crawl_name = \"CC-MAIN-2023-40\"\n",
    "top_lvl_domain = \"at\"   # oder \"de\"\n",
    "path1 = 'https://data.commoncrawl.org/cc-index/collections/'\n",
    "path2 = '/indexes/'\n",
    "path_ccrawl = path1 + crawl_name + path2\n",
    "url = path_ccrawl + 'cluster.idx'\n",
    "print(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:42:54.817657800Z",
     "start_time": "2024-02-17T18:42:54.782078200Z"
    }
   },
   "id": "50a8e33896a1aa4f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crawl_dir = os.path.join(\"S:\", \"msommer\", crawl_name, top_lvl_domain)\n",
    "if not os.path.exists(crawl_dir):\n",
    "   os.makedirs(crawl_dir)\n",
    "cluster_file = os.path.join(crawl_dir, \"cluster.txt\")\n",
    "#urlretrieve(url, cluster_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:42:54.877659500Z",
     "start_time": "2024-02-17T18:42:54.797025600Z"
    }
   },
   "id": "9b26717aeaca600e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cdx-00002.gz', 'cdx-00003.gz', 'cdx-00004.gz']\n"
     ]
    }
   ],
   "source": [
    "# 2) filter cdx files with tld '^at,' or '^de,'\n",
    "regex = '^' + top_lvl_domain + ','\n",
    "with open(cluster_file, \"rt\") as file:     \n",
    "    cdx_files = []\n",
    "    for line in file:\n",
    "        tld = line.split(\"\\t\")\n",
    "        match = re.search(regex, tld[0])  # ^at,\n",
    "        if match: \n",
    "            if not tld[1] in cdx_files:\n",
    "                cdx_files.append(tld[1])\n",
    "print(cdx_files)      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:43:04.248859500Z",
     "start_time": "2024-02-17T18:42:58.807509300Z"
    }
   },
   "id": "e7a1b3aa7f997d10",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3) randomly choose cdx files, e.g. cdx-00003.gz -> way too much for de\n",
    "if len(cdx_files) > 1:\n",
    "    idx = random.sample(range(0, len(cdx_files)), 1)\n",
    "    print(idx)\n",
    "    cdx_files = [cdx_files[i] for i in idx]\n",
    "    print(cdx_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T19:32:11.534934200Z",
     "start_time": "2024-02-17T19:32:11.521938Z"
    }
   },
   "id": "78dfbdb5cdf72da2",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdx-00003.gz\n"
     ]
    }
   ],
   "source": [
    "# 4) download cdx files (one takes ~ 2 min)\n",
    "# wget “https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2022-05/indexes/$i.gz\"\n",
    "for file in cdx_files:\n",
    "    print(file)\n",
    "    url = path_ccrawl + file\n",
    "    filename = os.path.join(crawl_dir, file)\n",
    "    #urlretrieve(url, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:43:15.377503600Z",
     "start_time": "2024-02-17T18:43:15.290780900Z"
    }
   },
   "id": "b7ac29262600bc36",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "def tryDownload(url, filename, retries=0):\n",
    "    if retries > 10: \n",
    "        print(\"Download failed.\")\n",
    "        return\n",
    "    try:\n",
    "        urlretrieve(url, filename)  \n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        tryDownload(url, filename, retries+1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T18:43:19.226088300Z",
     "start_time": "2024-02-17T18:43:19.150807700Z"
    }
   },
   "id": "2cbb9079c9e70970",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:msommer\\CC-MAIN-2023-40\\at\\cdx-00003.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(filename)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m gzip\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrt\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m----> 7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m:\u001B[49m\u001B[43m          \u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mregex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# ^at,\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\interdiscProject\\Lib\\gzip.py:314\u001B[0m, in \u001B[0;36mGzipFile.read1\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    313\u001B[0m     size \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[1;32m--> 314\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer\u001B[38;5;241m.\u001B[39mread1(size)\n",
      "File \u001B[1;32m~\\.conda\\envs\\interdiscProject\\Lib\\_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[1;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\interdiscProject\\Lib\\gzip.py:507\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;66;03m# Read a chunk of data from the file\u001B[39;00m\n\u001B[0;32m    505\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread(io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE)\n\u001B[1;32m--> 507\u001B[0m uncompress \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decompressor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecompress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munconsumed_tail \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    509\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mprepend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decompressor\u001B[38;5;241m.\u001B[39munconsumed_tail)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 5) gunzip and open it - one line looks like the following:\n",
    "warc_files = []\n",
    "for file in cdx_files:\n",
    "    filename = os.path.join(crawl_dir, file)\n",
    "    print(filename)\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        for line in f:          \n",
    "            match = re.search(regex, line)  # ^at,\n",
    "            if match: \n",
    "                l = line.split(\"{\\\"url\\\":\")\n",
    "                string = \"{\\\"url\\\":\" + l[1]\n",
    "                d = json.loads(string)\n",
    "                warc = d[\"filename\"]\n",
    "                if int(d[\"length\"]) > 10000: \n",
    "                    warc_files.append(warc)\n",
    "        count_dict = Counter(warc_files)    # 1 - 73 occurrences per warc file\n",
    "        \n",
    "        # only download warc files with a lot of occurrences --> maybe download not necessary? --> see below\n",
    "        wet_files = []\n",
    "        for key, value in count_dict.items():\n",
    "            if value >= 50:\n",
    "                # 6) get filename, convert to wet file and download (WARC: ~ 2 min, WET: 15 sec )\n",
    "                key = key.replace(\"/warc/\", \"/wet/\").replace(\"warc.gz\", \"warc.wet.gz\")\n",
    "                key_path = key.split(\"/\")[-1]\n",
    "                filename = os.path.join(crawl_dir, key_path)\n",
    "                wet_files.append(filename)\n",
    "                print(filename)\n",
    "                url = \"https://data.commoncrawl.org/\" + key\n",
    "                print(url)\n",
    "                #tryDownload(url, filename)  \n",
    "                break\n",
    "                \n",
    "    \n",
    "            \n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T19:32:45.418163600Z",
     "start_time": "2024-02-17T19:32:29.577533100Z"
    }
   },
   "id": "3217dcf181187af0",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "warcinfo\n",
    "request\n",
    "response\n",
    "metadata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f729021b4c03f96d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.oe1iah.at/Hardware/Antennen/Ofenrohr.shtml\n",
      "https://blog.lehofer.at/2016/07/Tele2.html?m=0\n",
      "https://industriemagazin.at/fertigen/maersk-nutzt-sap-business-technology-platform-zur-beschleunigung-des-strategischen-wandels/\n",
      "https://nksp.at/?shpxid=f8814aac-c894-4d69-aad5-031a08f37f93\n",
      "https://tv.skrapid.at/detail/list/295?videoId=18571\n",
      "https://www.akad-fernstudium.at/abschluss/weiterbildung/international-business-managerin-china-akad/\n",
      "https://www.beautygossip.at/2014/03/page/2/\n",
      "https://www.gear4music.at/de/Perkussion/Zubehoer/Glockenspiel-Zubehoer\n",
      "https://www.grazwiki.at/Rudolf-Hans-Bartsch-Stra%C3%9Fe_3\n",
      "https://www.haefele.at/de/produkt/tuerdruecker-garnitur-haefele-startec-modell-ldh-2171-edelstahl/P-00895232/\n",
      "https://www.ingastro.at/produkt/spind-800x450x1700-mm-2-turig/\n"
     ]
    }
   ],
   "source": [
    "wet_file = \"S:msommer\\CC-MAIN-2023-40\\\\at\\CC-MAIN-20231001041719-20231001071719-00247.warc.wet.gz\"\n",
    "fname = wet_file.replace(\".warc.wet.gz\", \"-text.txt\")\n",
    "with open(wet_file, 'rb') as stream, open(fname, 'wb') as f:\n",
    "    x = 0\n",
    "    for record in ArchiveIterator(stream):\n",
    "        if record.rec_type == 'conversion':\n",
    "            regex = '\\.' + top_lvl_domain + '/'\n",
    "            match = re.search(regex, record.rec_headers.get_header('WARC-Target-URI'))\n",
    "            length = int(record.rec_headers.get_header('Content-Length'))\n",
    "            rec_type = record.rec_headers.get_header('Content-Type')\n",
    "            if match and length > 10000 and rec_type == \"text/plain\":\n",
    "                print(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "                content = record.content_stream().read()\n",
    "                f.write(content)\n",
    "\n",
    "                x += 1\n",
    "        if x > 10:\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T19:14:36.618469Z",
     "start_time": "2024-02-17T19:14:29.331414Z"
    }
   },
   "id": "85603483f6104526",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### QUESTION: Wie löse ich das Problem, dass oft nur Überschriften im Text vorkommen?! \n",
    "# -> vlt beim einlesen von text file line per line die mit wenigen zeichen vernachlässigen\n",
    "### QUESTION: Wie stelle ich sicher, dass nur deutsche Webinhalte verwendet werden??\n",
    "# -> line per line language detection ?? -> sehr zeitaufwendig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49c8ad622c201fea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "fname = \"S:msommer\\CC-MAIN-2023-40\\\\at\\CC-MAIN-20231001041719-20231001071719-00247-text.txt\"\n",
    "final_fname = fname.replace(\"text.txt\", \"finaltext.txt\")\n",
    "with open(fname, \"rt\", encoding=\"utf-8\") as file, open(final_fname, 'wt', encoding=\"utf-8\") as f:\n",
    "    for line in file:\n",
    "        if line.count('.') < 2:\n",
    "            continue\n",
    "        try:\n",
    "            lang = detect(line)\n",
    "        except:\n",
    "            lang = \"none\"\n",
    "        if lang != \"de\":\n",
    "            continue\n",
    "        f.write(line) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T21:25:42.630937800Z",
     "start_time": "2024-02-17T21:25:35.986105600Z"
    }
   },
   "id": "c710c985e04e0e2b",
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://stackoverflow.com/questions/34612691/how-can-one-extract-every-payload-from-warc-wet-gz\n",
    "\n",
    "The code extracts the first 100 text \"payloads\" from the .warc.wet.gz file. The 100 created text files have shortened names which correspond to their associated url in the warc.wet.gz file. Many of the urls are too long to be used as as convenient names. The file wet_list_out.txt is generated and provides the correspondence between the shortened .txt file names and the associated urls.\n",
    "The code works. Of course you need to replace warc_wet_gz with the full path name of your own .warc.wet.gz file. If you want all the payloads, not just the first 100, set num to a very large number. Variable num was tacilty assumed to be 100 in the initial post. Why was it rated as not useful? The problem is solved. If you find issues with it please make specific suggestions. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4223fd069ad78e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from requests_file import FileAdapter\n",
    "def print_f_records(fpath,f_out,num):\n",
    "    record_count = 0\n",
    "    conversion_count = 0\n",
    "    header_count = 0\n",
    "    splitter = \"://\"\n",
    "    mx = num\n",
    "    s = requests.session()\n",
    "    s.mount('file://',FileAdapter())\n",
    "    resp = s.get(fpath, stream=True)\n",
    "    for record in ArchiveIterator(resp.raw, arc2warc=True):\n",
    "        record_count +=1\n",
    "        if record.rec_type == 'warcinfo':\n",
    "            print(record.raw_stream.read())\n",
    "        elif record.rec_type == 'conversion':\n",
    "            conversion_count +=1\n",
    "            if record.raw_stream.read is not None:\n",
    "                if record.rec_headers.get_header('Content-Type') == 'text/plain':\n",
    "                    header_count +=1\n",
    "                    if header_count > mx:\n",
    "                        continue\n",
    "                    print(\"\\n text/plain header no: \", header_count)\n",
    "                    prefix = f\"a{header_count}_\"        # filenames will begin with a<header_count>\n",
    "                    fname1 = (record.rec_headers.get_header('WARC-Target-URI'))\n",
    "                    fname = fname1.split(splitter)[1]   # get the name part which follows ://\n",
    "                    fname = prefix + fname[:22] + '.txt'    # keep entire file name to 32 characters or less\n",
    "                    fname = fname.replace(\"/\", \"_\")\n",
    "                    content = record.content_stream().read()\n",
    "                    f = open(fname, 'wb')               # note the wb; write in binay mode because the content is characters\n",
    "                    f.write(content)\n",
    "                    f.close()\n",
    "                    line = f'{fname1},{fname}' + \"\\n\" # keep a record of the url translations\n",
    "                    f_out.write(line)\n",
    "                    print(\"created text file: \", fname)\n",
    "                    if header_count == mx:\n",
    "                        print(f\"\\n... completed {mx} txt files ... \\n\")\n",
    "    print(\"Number of records: \", record_count)\n",
    "    print(\"Number of conversion types: \", conversion_count)\n",
    "    print(\"Number of text/plain http_headers: \", header_count)\n",
    "    print(\"\\nFINISHED\")\n",
    "warc_wet_gz ='file:////home/psl/CCrawl/text_extraction/text_from_wet/CC-MAIN-20230928063033-20230928093033-00593.warc.wet.gz'\n",
    "print(f'\\n warc.wet.gz file is {warc_wet_gz}')\n",
    "num=100\n",
    "f_out=open(\"wet_list_out.txt\", 'w')\n",
    "print_f_records(warc_wet_gz, f_out, num)\n",
    "f_out.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28765879e5d3b39c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "51018ba7d6f33d45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
