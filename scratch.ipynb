{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interdisciplinary Project\n",
    "by Marina Sommer | 11778902"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66af32a14aeb3ff3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Workflow is as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a34fa94bc89a283"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import packages\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import re \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:35:08.334135Z",
     "start_time": "2024-02-14T21:35:08.318605Z"
    }
   },
   "id": "417a92092b5e6dab",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2023-40/indexes/cluster.idx\n"
     ]
    }
   ],
   "source": [
    "# 1) select crawl: CC-MAIN-2023-40 and download cluster.idx\n",
    "crawl_name = \"CC-MAIN-2023-40\"\n",
    "path1 = 'https://data.commoncrawl.org/cc-index/collections/'\n",
    "path2 = '/indexes/'\n",
    "path_ccrawl = path1 + crawl_name + path2\n",
    "url = path_ccrawl + 'cluster.idx'\n",
    "print(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:35:12.294221700Z",
     "start_time": "2024-02-14T21:35:12.272709900Z"
    }
   },
   "id": "50a8e33896a1aa4f",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crawl_dir = os.path.join(\"S:\", \"msommer\", crawl_name)\n",
    "if not os.path.exists(crawl_dir):\n",
    "   os.makedirs(crawl_dir)\n",
    "cluster_file = os.path.join(crawl_dir, \"cluster.txt\")\n",
    "urlretrieve(url, cluster_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:36:03.813876Z",
     "start_time": "2024-02-14T21:36:03.780459100Z"
    }
   },
   "id": "9b26717aeaca600e",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cdx-00002.gz', 'cdx-00003.gz', 'cdx-00004.gz']\n"
     ]
    }
   ],
   "source": [
    "# 2) filter cdx files with tld '^at,' or '^de,'\n",
    "regex = '^at,'\n",
    "with open(cluster_file, \"rt\") as file:     \n",
    "    cdx_files = []\n",
    "    for line in file:\n",
    "        tld = line.split(\"\\t\")\n",
    "        match = re.search(regex, tld[0])  # ^at,\n",
    "        if match: \n",
    "            if not tld[1] in cdx_files:\n",
    "                cdx_files.append(tld[1])\n",
    "print(cdx_files)      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:36:14.140976700Z",
     "start_time": "2024-02-14T21:36:08.223605700Z"
    }
   },
   "id": "e7a1b3aa7f997d10",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3) randomly choose cdx files, e.g. cdx-00003.gz -> way too much for de\n",
    "if len(cdx_files) > 3:\n",
    "    idx = random.sample(range(0, len(cdx_files)), 3)\n",
    "    print(idx)\n",
    "    cdx_files = [cdx_files[i] for i in idx]\n",
    "    print(cdx_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:36:22.741295800Z",
     "start_time": "2024-02-14T21:36:22.721534100Z"
    }
   },
   "id": "78dfbdb5cdf72da2",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdx-00002.gz\n",
      "cdx-00003.gz\n",
      "cdx-00004.gz\n"
     ]
    }
   ],
   "source": [
    "# 4) download cdx files (one takes ~ 2 min)\n",
    "# wget â€œhttps://data.commoncrawl.org/cc-index/collections/CC-MAIN-2022-05/indexes/$i.gz\"\n",
    "for file in cdx_files:\n",
    "    print(file)\n",
    "    url = path_ccrawl + file\n",
    "    filename = os.path.join(crawl_dir, file)\n",
    "    urlretrieve(url, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:40:45.854176400Z",
     "start_time": "2024-02-14T21:36:27.422391700Z"
    }
   },
   "id": "b7ac29262600bc36",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 22: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 5) gunzip and open it - one line looks like the following:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrt\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:     \n\u001B[1;32m----> 3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mbreak\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\interdiscProject\\Lib\\encodings\\cp1252.py:23\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m codecs\u001B[38;5;241m.\u001B[39mcharmap_decode(\u001B[38;5;28minput\u001B[39m,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,decoding_table)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x8d in position 22: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# 5) gunzip and open it - one line looks like the following:\n",
    "with open(filename, \"rt\") as file:     \n",
    "    for line in file:\n",
    "        print(line)\n",
    "        break\n",
    "# gunzip(\"cdx-00003.gz\", remove=FALSE)\n",
    "# at,ff-asenham)/neuigkeiten.html?id=142 20231004230843 \n",
    "# {\"url\": \"http://ff-asenham.at/neuigkeiten.html?id=142\",\n",
    "# \"mime\": \"text/html\", \"mime-detected\": \"application/xhtml+xml\",\n",
    "# \"status\": \"200\", \"digest\": \"C7PN4ZGV7JOV33FXIMAU7VG7YC446N6W\",\n",
    "# \"length\": \"2508\", \"offset\": \"18047537\",\n",
    "# \"filename\": \"crawl-data/CC-MAIN-2023-40/segments/1695233511424.48/warc/CC-MAIN-20231004220037-20231005010037-00086.warc.gz\",\n",
    "# \"charset\": \"UTF-8\", \"languages\": \"deu\"}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:44:46.188710600Z",
     "start_time": "2024-02-14T21:44:46.122905600Z"
    }
   },
   "id": "3217dcf181187af0",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 6) get filename, convert to wet file, download (WARC: ~ 2 min, WET: 15 sec ) and gunzip\n",
    "# warc: wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-40/segments/1695233511424.48/warc/CC-MAIN-20231004220037-20231005010037-00086.warc.gz\n",
    "# wet:  wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-40/segments/1695233511424.48/wet/CC-MAIN-20231004220037-20231005010037-00086.warc.wet.gz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd25bd1e1eff76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### QUESTION: How to extract text content of each file?! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49c8ad622c201fea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
